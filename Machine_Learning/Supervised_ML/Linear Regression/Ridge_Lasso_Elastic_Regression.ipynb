{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will Discuss Ridge and Lasso Regression.\n",
    "\n",
    "So far, we haven't exactly used any specific techniques to evaluate the model that we are making. We will be discussing those as well. However, something that comes up during evaluation is the concept of overfitting and underfitting.\n",
    "\n",
    "**Overfitting:** This occurs when your model accurately predicts the training data but fails to achieve a similar level of accuracy on unseen data.\n",
    "\n",
    "**Underfitting:** This occurs when your model doesn't perform well on the training data.\n",
    "\n",
    "Underfitting can occur due to a small amount of data, the wrong algorithms used, or a learning rate that is too high or too low. It is fairly simple to address.\n",
    "\n",
    "The main problem arises when your model overfits. This happens when the model parameters exactly replicate a set of data points (e.g., if my data points are in an exponential shape). When new, unseen data is given to the model, which might deviate from the original graph found via training, the model predicts these data points poorly.\n",
    "\n",
    "To determine if your model is overfitting or underfitting, we mainly use two terms: `Bias` and `Variance`.\n",
    "\n",
    "**Bias** is simply the measure of the error made while making a prediction by our model.\n",
    "\n",
    "- Low bias means the error between our prediction and the target value is low.\n",
    "- High bias means this error is high.\n",
    "\n",
    "Obviously, a model with low bias is favorable.\n",
    "\n",
    "**Variance** specifies the amount our model varies when a different portion of the training dataset (or any dataset) is used.\n",
    "\n",
    "- Low variance means there is a small variation in the predictions for different portions of datasets.\n",
    "- High variance means there is a large variation.\n",
    "\n",
    "Ideally, we want our model to have low variance.\n",
    "\n",
    "Thus, an ideal model will have low bias and low variance (though this is not always possible, and we will soon talk about the bias-variance tradeoff).\n",
    "\n",
    "A model with low bias but high variance is overfitting, and a model with high bias and low variance is underfitting. (If both are high, it is chaos.)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Ridge and Lasso Regression** help solve the problem of overfitting in Linear Regression by introducing a regularization parameter. (Additionally, Lasso also helps in reducing the dimensionality of our data.)\n",
    "\n",
    "Starting with **Ridge Regression** also called **L2 Regularization**\n",
    "\n",
    "Regularization : it is a set of methods used for reducing overfitting in ML models.\n",
    "\n",
    "In `Ridge Regression`, while calculating the Mean Squared Error (MSE), we basically add a new term:-\n",
    "\n",
    "$$ \\text{MSE}_{\\text{ridge}} = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 + \\frac{\\lambda}{2p} \\sum_{j=1}^{p} w_j^2 $$\n",
    "\n",
    "where:\n",
    "- $ y_i $ are the observed values,\n",
    "- $ \\hat{y}_i $ are the predicted values,\n",
    "- $ \\lambda $ is the regularization parameter,\n",
    "- $ W $ or $ \\beta_j $ are the coefficients (slopes).\n",
    "- $ p $ is number of features or coefficients\n",
    "\n",
    "The additional term $ \\lambda \\sum_{j=1}^{p} w_j^2 $ helps to penalize large coefficients, thereby reducing overfitting.\n",
    "\n",
    "Adding this parameter introduces a new error term, which makes the predictions slightly less accurate on the training data but results in lower variance overall. This means that although it reduces the accuracy on the training data, it compensates by improving the model's performance on new, unseen data.\n",
    "\n",
    "In simpler words, if our model creates a line that fits our data perfectly, introducing Ridge's term will result in a line that isn't perfect. This increases the MSE from 0 to something higher, but it helps generalize the model, leading to better predictions on new, unseen data.\n",
    "(we will vizualize this below.)\n",
    "\n",
    "Hence Solving the problemn of Over Fitting.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Lasso Regression** also called **L1 Regularization**\n",
    " \n",
    "Similar to Ridge Regression, `Lasso Regression` also introduces a term to the MSE:\n",
    "\n",
    "$$ \\text{MSE}_{\\text{Lasso}} = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right)^2 + \\frac{\\lambda}{p} \\sum_{j=1}^{p} \\lvert w_j \\rvert$$\n",
    "\n",
    "Where everything means the same as in Ridge Regression, only here we take the absolute sum of the slopes.\n",
    "\n",
    "In addition to rectifying the problem of overfitting in the same way as Ridge Regression, Lasso Regression also helps in reducing the dimensionality of our data. This means that it can effectively identify and eliminate irrelevant features.\n",
    "\n",
    "---\n",
    "\n",
    "###### Now How Does this Eleminate the irrelevant Features?, well It is more of a Mathematical Reason why this happens and i will understand and explain  this on pen and paper.\n",
    "\n",
    "\n",
    "### Ridge Regression:-\n",
    "\n",
    "The derivative of the regularization term $ \\frac{\\lambda}{2p} \\sum_{j=1}^{p} w_j^2 $ with respect to $ \\mathbf{w} $ in Ridge regression is computed as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i} \\left( \\frac{\\lambda}{2p} \\sum_{j=1}^{p} w_j^2 \\right) = \\frac{2 \\lambda}{2p} w_i = \\frac{\\lambda}{p} w_i $$\n",
    "\n",
    "Therefore, the derivative of the regularization term $ \\frac{\\lambda}{2p} \\sum_{j=1}^{p} w_j^2 $ with respect to $ w_i $ is $ \\frac{\\lambda}{p} w_i $. This derivative is used in Ridge regression to adjust the gradient descent update step for the weight $ w_i $, helping to regularize the model and control overfitting. (Nothing changes for $ b $).\n",
    "\n",
    "### Lasso Regression:-\n",
    "\n",
    "The derivative of the regularization term $ \\lambda \\sum_{j=1}^{p} |w_j| $ with respect to $ w_j $ in Lasso regression is computed as follows:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_i} (\\lambda \\sum_{j=1}^{p} |w_j|) = \\lambda \\cdot \\text{sign}(w_j) $$\n",
    "\n",
    "where $ \\text{sign}(w_j) $ is the sign function, defined as:\n",
    "\n",
    "$$ \\text{sign}(w_j) = \\begin{cases}\n",
    "1 & \\text{if } w_j > 0 \\\\\n",
    "-1 & \\text{if } w_j < 0 \\\\\n",
    "0 & \\text{if } w_j = 0\n",
    "\\end{cases} $$\n",
    "\n",
    "Therefore, the derivative of the regularization term $ \\lambda \\sum_{j=1}^{p} |w_j| $ with respect to $ w_j $ is $ \\lambda \\cdot \\text{sign}(w_j) $. This derivative is used in Lasso regression to adjust the gradient descent update step for the weight $ w_j $, promoting sparsity by potentially driving some weights $ w_i $ to zero, which effectively eliminates irrelevant features.\n",
    "\n",
    "##### Note: The $ |X| $ function isn't differentiable, but for the purpose of machine learning algorithms, we use its subderivative. I just learned about this concept and will be exploring it further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code implementation of Ridge\n",
    "\n",
    "def compute_cost_ridge(X,Y,W,b,L):\n",
    "    \"\"\"\n",
    "    This function computes the Mean Squared Error (MSE) \n",
    "    of our model on the training data.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input values\n",
    "        Y (ndarray): Actual values or target values\n",
    "        W (ndarray): Weights for the input parameters\n",
    "        b (scalar) : Intercept or bias term\n",
    "        L (scalar) : Regularizaton Parameter\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The Mean Squared Error (MSE)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    yhat = np.dot(X,W) + b\n",
    "\n",
    "    sum_w = np.sum(W**2) #This computes the square of the weights and sums them.\n",
    "\n",
    "    for i in range(m):\n",
    "        cost += (yhat[i] - Y[i]) ** 2 \n",
    "\n",
    "    total_cost = (cost + L*sum_w) / (2 * m)\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def compute_gradient_ridge(X,Y,W,b,L):\n",
    "    \"\"\"\n",
    "    This function computes the Gradient of cost fuction\n",
    "    for a given set of w and b values.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray) : Training Input values\n",
    "        Y (ndarray) : Target value or (output values for the input)    \n",
    "        W (ndarray) : slope or wrights for the input parameter\n",
    "        b (scalar) : intercept or bias parameter\n",
    "        L (scalar) : Regularizaton Parameter\n",
    "\n",
    "    Returns:\n",
    "        dj_dw (ndarray) : gradient when partially diffrentiated wrt w\n",
    "        dj_db (ndarray) : gradient when partially diffrentiated wrt b\n",
    "    \"\"\"\n",
    "        \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros(n,)\n",
    "    dj_db = 0.0\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i],W) + b) - Y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err *  X[i,j] \n",
    "        dj_db += err \n",
    "\n",
    "    dj_dw = dj_dw/m + (L/m)*W   #This add the regularization term.\n",
    "    dj_db /= m\n",
    "\n",
    "    return dj_dw,dj_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_lasso(X, Y, W, b, L):\n",
    "    \"\"\"\n",
    "    This function computes the Mean Squared Error (MSE) \n",
    "    of our model on the training data.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input values\n",
    "        Y (ndarray): Actual values or target values\n",
    "        W (ndarray): Weights for the input parameters\n",
    "        b (scalar): Intercept or bias term\n",
    "        L (scalar): Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The Mean Squared Error (MSE)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    yhat = np.dot(X, W) + b\n",
    "\n",
    "    sum_w = np.sum(abs(W))  # This computes the absolute value of the weights and sums them.\n",
    "\n",
    "    for i in range(m):\n",
    "        cost += (yhat[i] - Y[i]) ** 2 \n",
    "\n",
    "    total_cost = (cost / (2 * m)) + (L / m) * sum_w\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def compute_gradient_lasso(X, Y, W, b, L):\n",
    "    \"\"\"\n",
    "    This function computes the Gradient of the cost function\n",
    "    for a given set of W and b values.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Training input values\n",
    "        Y (ndarray): Target values or output values for the input    \n",
    "        W (ndarray): Weights for the input parameters\n",
    "        b (scalar): Intercept or bias parameter\n",
    "        L (scalar): Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        dj_dw (ndarray): Gradient when partially differentiated with respect to W\n",
    "        dj_db (ndarray): Gradient when partially differentiated with respect to b\n",
    "    \"\"\"\n",
    "        \n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], W) + b) - Y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * X[i, j]\n",
    "        dj_db += err \n",
    "\n",
    "    dj_dw = (dj_dw / m) + (L / m) * np.sign(W)  # This adds the regularization term.\n",
    "    dj_db /= m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have understood what these mean and how to implement them, there is something called Elastic Regression which combines both Ridge and Lasso regularization. \n",
    "\n",
    "And I mean literally, this is the MSE in Elastic Net Regression:\n",
    "\n",
    "$$ \\text{MSE}_{\\text{Elastic Net}} = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right)^2 + \\frac{\\lambda_1}{m} \\sum_{j=1}^{m} \\lvert w_j \\rvert + \\frac{\\lambda_2}{2m} \\sum_{j=1}^{m} w_j^2 $$\n",
    "\n",
    "Here:\n",
    "- $ m $ is the number of samples.\n",
    "- $ n $ is the number of features.\n",
    "- $ y_i $ represents the actual output for the $ i $th sample.\n",
    "- $ \\hat{y}_i $ represents the predicted output for the $ i $th sample.\n",
    "- $ \\lambda_1 $ and $ \\lambda_2 $ are regularization parameters controlling the strengths of Lasso (L1) and Ridge (L2) regularization, respectively.\n",
    "\n",
    "This formulation combines Lasso and Ridge penalties to achieve both feature selection and regularization in the Elastic Net Regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative of Elastic Net Regression MSE\n",
    "\n",
    "The derivative of the Elastic Net Regression MSE with respect to $ w_i $ is:\n",
    "\n",
    "$$ \\frac{\\partial \\text{MSE}_{\\text{Elastic Net}}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right) x_{ij} + \\frac{\\lambda_2}{m} w_j + \\frac{\\lambda_1}{m} \\cdot \\text{sign}(w_j) $$\n",
    "\n",
    "\n",
    "\n",
    "where:\n",
    "- $ m $ is the number of samples.\n",
    "- $ y_i $ represents the actual output for the $ i $ th sample.\n",
    "- $ \\hat{y}_i $ represents the predicted output for the $ i $ th sample.\n",
    "- $ x_{i,j} $ is the $ j $ th feature value of the $ i $ th sample.\n",
    "- $ \\lambda_1 $ and $ \\lambda_2 $ are regularization parameters controlling the strengths of Lasso (L1) and Ridge (L2) regularization, respectively.\n",
    "- $ \\text{sign}(w_i) $ is the sign function defined as:\n",
    "  $$ \\text{sign}(w_i) = \\begin{cases}\n",
    "  1 & \\text{if } w_i > 0 \\\\\n",
    "  -1 & \\text{if } w_i < 0 \\\\\n",
    "  0 & \\text{if } w_i = 0\n",
    "  \\end{cases} $$\n",
    "\n",
    "This derivative helps in computing the gradient during the optimization process of Elastic Net Regression, adjusting the weights $ w_i $ accordingly to minimize the MSE while considering both Lasso and Ridge regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_elastic_net(X, Y, W, b, L1, L2):\n",
    "    \"\"\"\n",
    "    This function computes the Mean Squared Error (MSE) \n",
    "    of our model on the training data.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Input values\n",
    "        Y (ndarray): Actual values or target values\n",
    "        W (ndarray): Weights for the input parameters\n",
    "        b (scalar): Intercept or bias term\n",
    "        L1 (scalar): L1 Regularization parameter\n",
    "        L2 (scalar): L2 Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The Mean Squared Error (MSE)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    yhat = np.dot(X, W) + b\n",
    "\n",
    "    sum_w_absolute = np.sum(np.abs(W))  \n",
    "    sum_w_squared = np.sum(W**2)  \n",
    "\n",
    "    for i in range(m):\n",
    "        cost += (yhat[i] - Y[i]) ** 2 \n",
    "\n",
    "    total_cost = (cost / (2 * m)) + (L1 / m) * sum_w_absolute + (L2 / (2 * m)) * sum_w_squared  \n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def compute_gradient_elastic_net(X, Y, W, b, L1, L2):\n",
    "    \"\"\"\n",
    "    This function computes the Gradient of the cost function\n",
    "    for a given set of W and b values.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Training input values\n",
    "        Y (ndarray): Target values or output values for the input    \n",
    "        W (ndarray): Weights for the input parameters\n",
    "        b (scalar): Intercept or bias parameter\n",
    "        L1 (scalar): L1 Regularization parameter\n",
    "        L2 (scalar): L2 Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        dj_dw (ndarray): Gradient when partially differentiated with respect to W\n",
    "        dj_db (ndarray): Gradient when partially differentiated with respect to b\n",
    "    \"\"\"\n",
    "        \n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], W) + b) - Y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * X[i, j]\n",
    "        dj_db += err \n",
    "\n",
    "    dj_dw = (dj_dw / m) + (L1 / m) * np.sign(W) + (L2 / m) * W  \n",
    "    dj_db /= m\n",
    "\n",
    "    return dj_dw, dj_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When I previously talked about Bias and Variance, I mentioned the concept of Bias and Variance Trade-off:\n",
    "It simply means that the more you try to lower the Bias, the more Variance increases. Similarly, the more you try to lower the Variance, the higher Bias goes.\n",
    "\n",
    "The key idea is that we need to find the optimal balance where both of these factors are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
