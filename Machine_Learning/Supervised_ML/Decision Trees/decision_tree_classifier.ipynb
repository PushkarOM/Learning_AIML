{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "A **Decision Tree** is a supervised machine learning algorithm that can be used for both classification and regression tasks. They are a visual representation of the decision-making process. The graph basically looks like an upside-down tree, with the root node at the top, and then it goes down, splitting into different branches and leaves.\n",
    "\n",
    "Some important terms:\n",
    "\n",
    "- **Root Node**: The top node of a decision tree from which all branches originate. It's the starting point.\n",
    "- **Leaf Node**: The end node of a decision tree where no further splitting occurs. They provide the final prediction.\n",
    "- **Internal Node**: The nodes between the root and leaf nodes. These nodes are used for making decisions and can further split into either leaf nodes or more internal nodes.\n",
    "- **Branch**: A path from the root to a leaf node. It represents the decisions made to reach a leaf node based on features.\n",
    "- **Pruning**: The process of trimming unnecessary nodes to optimize the decision tree.\n",
    "\n",
    "Decision trees break down the data into smaller and smaller subsets via if-else-like conditions, which eventually lead to a leaf node, i.e., a set of predictions. After a decision tree has been implemented, the final tree is also called a set of decision rules. By following these decision rules, predictions on new unseen data can be made.\n",
    "\n",
    "#### CART Algorithm\n",
    "\n",
    "There are two prominent algorithms that utilize decision trees for classification tasks: **CART (Classification and Regression Trees)** and **ID3 (Iterative Dichotomiser 3)**. We will first explore CART specifically for classification tasks, then discuss CART for regression tasks, and finally examine ID3.\n",
    "\n",
    "\n",
    "How does CART Perform the Split :\n",
    "CART follows a *greedy approach*. The algorithm starts with the entire dataset and calculates the best way to split the data. It selects the best split by minimizing the **Gini impurity** (aka GINI Index or Metric) or finding the maximum value of **Information Gain**.\n",
    "\n",
    "**Gini Index (or Metric):** The Gini index measures the probability of a random instance being misclassified when chosen randomly. In other words, it measures how mixed or impure the data is (if the data is highly impure, the chances of picking a random instance and it being misclassified are high). Its value ranges from 0 to 1, where 0 indicates a pure dataset and 1 indicates an impure dataset.\n",
    "\n",
    "The formula goes like :\n",
    "For a node $ t $ with $ m $ classes, the Gini impurity $ G(t) $ is calculated as:\n",
    "\n",
    "$$\n",
    "G(t) = 1 - \\sum_{i=1}^{m} p_i^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ p_i $ is the proportion of instances in the node that belong to class $ i $.\n",
    "- $ m $ is the total number of classes.\n",
    "\n",
    "**Information Gain:** It measures the increase in information from one state to another. Specifically, it checks if splitting on a particular feature will yield a higher information gain. It does this by subtracting the weighted average of the Gini impurity of all subsets created after splitting the tree from the Gini impurity of the parent node. In other words, it checks if the data becomes more pure or impure after splitting. If the data becomes more pure, the split is considered beneficial and is performed.\n",
    "\n",
    "$$\n",
    "\\text{Information Gain (Gini)} = \\text{Gini}(S) - \\text{Gini}(S, A)\n",
    "$$\n",
    "\n",
    "Were:\n",
    "- $\\text{Gini}(S)$ is the Gini impurity of the original dataset\n",
    "- $\\text{Gini}(S, A)$ is the weighted average Gini impurity of the subsets created by splitting on feature $A$.\n",
    "\n",
    "\n",
    "The split is performed by asking binary questions like \"Does this instance belong to this class?\" with answers being either YES or NO. Thus, CART with the Gini index performs a binary split on the data. It starts by calculating the Gini impurity of the entire dataset. Then, it performs splits using all the different features. For each split, it calculates the Gini impurity of the resulting child nodes and computes their weighted average. The Algortihm then calculates the information gain. Then the split that results in the highest Information Gain is selected and performed.\n",
    "\n",
    "This splitting process occurs recursively until the data becomes pure (i.e., the Gini impurity of a node reaches 0 or informatoin gain becomes 0) for all the nodes that are created (or a stopping criterior is reached such as max tree depth). These final nodes, where the data is *pure*, represent the final predictions for specific inputs.\n",
    "\n",
    "The **Information Gain** we just read is Also Performed using something called **Entropy**, more on this later.\n",
    "\n",
    "#### Let's implement a Basic Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values:\n",
      "      Outlook  Temp Humidity Windy\n",
      "0      Rainy   Hot     High     f\n",
      "1      Rainy   Hot     High     t\n",
      "2   Overcast   Hot     High     f\n",
      "3      Sunny  Mild     High     f\n",
      "4      Sunny  Cool   Normal     f\n",
      "5      Sunny  Cool   Normal     t\n",
      "6   Overcast  Cool   Normal     t\n",
      "7      Rainy  Mild     High     f\n",
      "8      Rainy  Cool   Normal     f\n",
      "9      Sunny  Mild   Normal     f\n",
      "10     Rainy  Mild   Normal     t\n",
      "11  Overcast  Mild     High     t\n",
      "12  Overcast   Hot   Normal     f\n",
      "13     Sunny  Mild     High     t\n",
      "Output values:\n",
      " 0     0\n",
      "1     0\n",
      "2     1\n",
      "3     1\n",
      "4     1\n",
      "5     0\n",
      "6     1\n",
      "7     0\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    1\n",
      "12    1\n",
      "13    0\n",
      "Name: Play, dtype: int64\n",
      "     Outlook  Temp Humidity Windy  Play\n",
      "0      Rainy   Hot     High     f     0\n",
      "1      Rainy   Hot     High     t     0\n",
      "2   Overcast   Hot     High     f     1\n",
      "3      Sunny  Mild     High     f     1\n",
      "4      Sunny  Cool   Normal     f     1\n",
      "5      Sunny  Cool   Normal     t     0\n",
      "6   Overcast  Cool   Normal     t     1\n",
      "7      Rainy  Mild     High     f     0\n",
      "8      Rainy  Cool   Normal     f     1\n",
      "9      Sunny  Mild   Normal     f     1\n",
      "10     Rainy  Mild   Normal     t     1\n",
      "11  Overcast  Mild     High     t     1\n",
      "12  Overcast   Hot   Normal     f     1\n",
      "13     Sunny  Mild     High     t     0\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ['Rainy', 'Hot', 'High', 'f', 'no'],\n",
    "    ['Rainy', 'Hot', 'High', 't', 'no'],\n",
    "    ['Overcast', 'Hot', 'High', 'f', 'yes'],\n",
    "    ['Sunny', 'Mild', 'High', 'f', 'yes'],\n",
    "    ['Sunny', 'Cool', 'Normal', 'f', 'yes'],\n",
    "    ['Sunny', 'Cool', 'Normal', 't', 'no'],\n",
    "    ['Overcast', 'Cool', 'Normal', 't', 'yes'],\n",
    "    ['Rainy', 'Mild', 'High', 'f', 'no'],\n",
    "    ['Rainy', 'Cool', 'Normal', 'f', 'yes'],\n",
    "    ['Sunny', 'Mild', 'Normal', 'f', 'yes'],\n",
    "    ['Rainy', 'Mild', 'Normal', 't', 'yes'],\n",
    "    ['Overcast', 'Mild', 'High', 't', 'yes'],\n",
    "    ['Overcast', 'Hot', 'Normal', 'f', 'yes'],\n",
    "    ['Sunny', 'Mild', 'High', 't', 'no']\n",
    "]\n",
    "\n",
    "dataset = pd.DataFrame(data)\n",
    "dataset.columns = ['Outlook','Temp','Humidity','Windy','Play']\n",
    "# Mapping dictionary\n",
    "mapping = {'yes': 1, 'no': 0}\n",
    "# Convert the 'Play' column\n",
    "dataset['Play'] = dataset['Play'].map(mapping)\n",
    "\n",
    "X = dataset.drop('Play',axis = 1)\n",
    "\n",
    "y = dataset['Play']\n",
    "\n",
    "print(\"Input values:\\n\",X)\n",
    "print(\"Output values:\\n\",y)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looks Familier right ?\n",
    "#### First, we need to implement the GINI and Inforamtion Gain Calculator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index_calc(Y):\n",
    "    \"\"\"\n",
    "    This function takes the data of a given node and split it according to GINI index.\n",
    "\n",
    "    Args:\n",
    "        Y (ndarray) : target values\n",
    "\n",
    "    returns:\n",
    "        gini_impurity (scalar) : GINI index value\n",
    "    \"\"\"\n",
    "    unique_target_vals = np.unique(Y)\n",
    "    gini_sum = 0\n",
    "\n",
    "    for cls in unique_target_vals:\n",
    "        gini_sum += (len(Y[Y == cls])/len(Y))**2\n",
    "\n",
    "    gini_impurity = 1 - gini_sum\n",
    "    return gini_impurity\n",
    "\n",
    "\n",
    "\n",
    "def information_gain(parent, l_child, r_child):\n",
    "    \"\"\"\n",
    "    This function computes the Information Gain for a particluar split\n",
    "    \n",
    "    Args:\n",
    "        parent (ndarray) : Y values of Parent Node\n",
    "        l_child (ndarray) : Y values of Left Child Node\n",
    "        r_child (ndarray) : Y values of right Child Node\n",
    "\n",
    "    Returns:\n",
    "        gain_value (float) : Information Gain\n",
    "    \"\"\"\n",
    "    gain_value = gini_index_calc(parent)\n",
    "    \n",
    "    weight_left = len(l_child) / len(parent)\n",
    "    weight_right = len(r_child) / len(parent)\n",
    "    \n",
    "    left_gini = gini_index_calc(l_child)\n",
    "    right_gini = gini_index_calc(r_child)\n",
    "    \n",
    "    gain_value -= (weight_left * left_gini + weight_right * right_gini)\n",
    "\n",
    "    return gain_value,left_gini,right_gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we will implement the Split dataset function and function to find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data,feature,value):\n",
    "    \"\"\"\n",
    "    Splits the dataset according to the feature and it's value\n",
    "\n",
    "    Args:\n",
    "        data (pd.dataframe) : input data\n",
    "        feature (string) : feature to split with\n",
    "        value (string) : specific value in that feature to split with\n",
    "\n",
    "    Returns:\n",
    "        left_split (pd.dataframe) : left split of dataset\n",
    "        right_split (pd.dataframe) : right split of dataset\n",
    "    \"\"\"\n",
    "    # Left split\n",
    "    left_split = data[(data[feature] == value)]\n",
    "    \n",
    "    # Right split\n",
    "    right_split = data[(data[feature] != value)]\n",
    "    \n",
    "    return left_split, right_split\n",
    "\n",
    "def find_best_split(data):\n",
    "    \"\"\"\n",
    "    Finds the best feature to split the dataset\n",
    "\n",
    "    Args:\n",
    "        data (pd.dataframe) : dataset\n",
    "        \n",
    "    Returns:\n",
    "        best_split (Dict) : dictionari containg all the relevant info about the best feature to split with\n",
    "    \"\"\"\n",
    "\n",
    "    best_split = {}\n",
    "    best_split['info_gain'] = 0\n",
    "    features = data.columns\n",
    "\n",
    "    target = data.columns[-1]\n",
    "\n",
    "    for feature in features:\n",
    "        #skip the iteration where the feature is equal to the target value\n",
    "        if feature == target:\n",
    "            continue\n",
    "\n",
    "        feature_values = data[feature].unique()\n",
    " \n",
    "        for feature_value in feature_values:\n",
    "            \n",
    "            parent_y = data[target].values\n",
    "            \n",
    "            #spliting the dataset in to nodes to find info gain from the split\n",
    "            left_split,right_split = split_dataset(data,feature,feature_value)\n",
    "            \n",
    "            #left split is the case of feature_value, we will now extract the target value in this case\n",
    "            y_left = left_split[target].values \n",
    "            \n",
    "            #right split is the case all the other values\n",
    "            y_right = right_split[target].values\n",
    "            \n",
    "\n",
    "            info_gain,left_gini,right_gini = information_gain(parent_y,y_left,y_right) #this will give the info gain of the current split\n",
    "            #print(info_gain,feature,feature_value)\n",
    "            if info_gain > best_split['info_gain'] :\n",
    "                best_split['info_gain'] = info_gain\n",
    "                best_split['feature'] = feature\n",
    "                best_split['value'] = feature_value\n",
    "                best_split['left_gini'] = left_gini #for understanding how leaf node is made\n",
    "                best_split['right_gini'] = right_gini\n",
    "                \n",
    "\n",
    "    return best_split\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'info_gain': 0.10204081632653056, 'feature': 'Outlook', 'value': 'Overcast', 'left_gini': 0.0, 'right_gini': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#testing our function are working properly : the output should be Overcast.\n",
    "print(find_best_split(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's implement the Algo for running this in recursion and building the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to create a TreeNode with a left and right node\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, left=None, right=None, is_leaf=False, prediction=None):\n",
    "        \"\"\"\n",
    "        Constructor Function\n",
    "        \"\"\"\n",
    "        self.feature = feature        # Feature to split on\n",
    "        self.value = value            # Feature's Value to split on\n",
    "        self.left = left              # Left child\n",
    "        self.right = right            # Right child\n",
    "        self.is_leaf = is_leaf        # Whether this node is a leaf node\n",
    "        self.prediction = prediction  # Prediction if this node is a leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, depth=0, max_depth=None):\n",
    "    \"\"\"\n",
    "    Recursively builds a decision tree from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataset with features and target.\n",
    "        depth (int): Current depth of the tree.\n",
    "        max_depth (int): Maximum depth of the tree.\n",
    "    \n",
    "    Returns:\n",
    "        TreeNode: The root node of the decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting target column\n",
    "    target = data.columns[-1]\n",
    "    \n",
    "    # if all examples have the same label (this basically checks if the gini is 0)\n",
    "    if len(data[target].unique()) == 1:\n",
    "        return TreeNode(is_leaf=True, prediction=data[target].iloc[0])\n",
    "    \n",
    "    # if there are no features left to split on or if max depth is reached (this checks the constraints)\n",
    "    if len(data.columns) == 1 or (max_depth is not None and depth >= max_depth):\n",
    "        majority_class = data[target].mode()[0]\n",
    "        return TreeNode(is_leaf=True, prediction=majority_class)\n",
    "    \n",
    "    # Find the best split\n",
    "    best_split = find_best_split(data)\n",
    "\n",
    "    # Split the data\n",
    "    left_split, right_split = split_dataset(data, best_split['feature'], best_split['value'])\n",
    "    \n",
    "    # Recursively build the left and right subtrees\n",
    "    left_node = build_tree(left_split, depth + 1, max_depth)\n",
    "    right_node = build_tree(right_split, depth + 1, max_depth)\n",
    "    \n",
    "    # Return the decision tree node\n",
    "    return TreeNode(\n",
    "        feature=best_split['feature'],\n",
    "        value=best_split['value'],\n",
    "        left=left_node,\n",
    "        right=right_node\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: Split on 'Outlook' <==== Overcast\n",
      "    Left:\n",
      "    Leaf: Predict 1\n",
      "    Right:\n",
      "    Node: Split on 'Humidity' <==== High\n",
      "        Left:\n",
      "        Node: Split on 'Outlook' <==== Rainy\n",
      "            Left:\n",
      "            Leaf: Predict 0\n",
      "            Right:\n",
      "            Node: Split on 'Windy' <==== f\n",
      "                Left:\n",
      "                Leaf: Predict 1\n",
      "                Right:\n",
      "                Leaf: Predict 0\n",
      "        Right:\n",
      "        Node: Split on 'Windy' <==== f\n",
      "            Left:\n",
      "            Leaf: Predict 1\n",
      "            Right:\n",
      "            Node: Split on 'Outlook' <==== Sunny\n",
      "                Left:\n",
      "                Leaf: Predict 0\n",
      "                Right:\n",
      "                Leaf: Predict 1\n"
     ]
    }
   ],
   "source": [
    "# Defining the Root Node and Building the Tree\n",
    "tree_root = build_tree(dataset)\n",
    "\n",
    "\n",
    "#Function to Print the Tree\n",
    "def print_tree(node, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the decision tree.\n",
    "    \n",
    "    Args:\n",
    "        node (TreeNode): The current node in the tree.\n",
    "        depth (int): The current depth of the node in the tree.\n",
    "    \"\"\"\n",
    "    # Print the current node's details\n",
    "    if node.is_leaf:\n",
    "        print(f\"{'  ' * 2 * depth}Leaf: Predict {node.prediction}\")\n",
    "    else:\n",
    "        print(f\"{'  ' * 2 * depth}Node: Split on '{node.feature}' <==== {node.value}\")\n",
    "        print(f\"{'  ' * 2 * (depth + 1)}Left:\")\n",
    "        print_tree(node.left, depth + 1)\n",
    "        print(f\"{'  ' * 2 *(depth + 1)}Right:\")\n",
    "        print_tree(node.right, depth + 1)\n",
    "\n",
    "\n",
    "print_tree(tree_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To visualize the decision tree effectively, we can use libraries like graphviz and sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "# Define the dataset\n",
    "data = [\n",
    "    ['Rainy', 'Hot', 'High', 'f', 'no'],\n",
    "    ['Rainy', 'Hot', 'High', 't', 'no'],\n",
    "    ['Overcast', 'Hot', 'High', 'f', 'yes'],\n",
    "    ['Sunny', 'Mild', 'High', 'f', 'yes'],\n",
    "    ['Sunny', 'Cool', 'Normal', 'f', 'yes'],\n",
    "    ['Sunny', 'Cool', 'Normal', 't', 'no'],\n",
    "    ['Overcast', 'Cool', 'Normal', 't', 'yes'],\n",
    "    ['Rainy', 'Mild', 'High', 'f', 'no'],\n",
    "    ['Rainy', 'Cool', 'Normal', 'f', 'yes'],\n",
    "    ['Sunny', 'Mild', 'Normal', 'f', 'yes'],\n",
    "    ['Rainy', 'Mild', 'Normal', 't', 'yes'],\n",
    "    ['Overcast', 'Mild', 'High', 't', 'yes'],\n",
    "    ['Overcast', 'Hot', 'Normal', 'f', 'yes'],\n",
    "    ['Sunny', 'Mild', 'High', 't', 'no']\n",
    "]\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['Outlook', 'Temperature', 'Humidity', 'Windy', 'PlayTennis'])\n",
    "\n",
    "# Define features and labels\n",
    "X = df[['Outlook', 'Temperature', 'Humidity', 'Windy']]\n",
    "y = df['PlayTennis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook  Temperature  Humidity  Windy  PlayTennis\n",
      "0         1            1         0      0           0\n",
      "1         1            1         0      1           0\n",
      "2         0            1         0      0           1\n",
      "3         2            2         0      0           1\n",
      "4         2            0         1      0           1\n",
      "5         2            0         1      1           0\n",
      "6         0            0         1      1           1\n",
      "7         1            2         0      0           0\n",
      "8         1            0         1      0           1\n",
      "9         2            2         1      0           1\n",
      "10        1            2         1      1           1\n",
      "11        0            2         0      1           1\n",
      "12        0            1         1      0           1\n",
      "13        2            2         0      1           0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoders\n",
    "le_outlook = LabelEncoder()\n",
    "le_temp = LabelEncoder()\n",
    "le_humidity = LabelEncoder()\n",
    "le_windy = LabelEncoder()\n",
    "le_play = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "df['Outlook'] = le_outlook.fit_transform(df['Outlook'])\n",
    "df['Temperature'] = le_temp.fit_transform(df['Temperature'])\n",
    "df['Humidity'] = le_humidity.fit_transform(df['Humidity'])\n",
    "df['Windy'] = le_windy.fit_transform(df['Windy'])\n",
    "df['PlayTennis'] = le_play.fit_transform(df['PlayTennis'])\n",
    "print(df)\n",
    "# Define features and labels again with encoded values\n",
    "X = df[['Outlook', 'Temperature', 'Humidity', 'Windy']]\n",
    "y = df['PlayTennis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the decision tree classifier\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images\\\\decision_tree.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the tree as a DOT file\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=['Outlook', 'Temperature', 'Humidity', 'Windy'],  \n",
    "                                class_names=le_play.classes_,  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True)  \n",
    "\n",
    "# Create a graph from the DOT data\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"./images/decision_tree\", format='png') # Save and display the tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](./images/decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a slight difference between my decision tree and the tree created using `sklearn`. In my tree, the final right node is based on the feature \"Outlook\" with the value \"Sunny.\" However, in the `sklearn` tree, the final right node is based on the feature \"Temperature.\" This is likely due to differences in the implementation of the decision tree algorithm between the two methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, sample):\n",
    "    \"\"\"\n",
    "    Predict the class label for a given sample.\n",
    "    \n",
    "    Args:\n",
    "        node (TreeNode) : Current TreeNode.\n",
    "        sample (Input Feature): Dictionary with feature values.\n",
    "    Returns: \n",
    "        Prediction (class label).\n",
    "    \"\"\"\n",
    "    if node.is_leaf:\n",
    "        return node.prediction\n",
    "    else:\n",
    "        feature_value = sample.get(node.feature)\n",
    "        if feature_value == node.value:\n",
    "            return predict(node.left, sample) if node.left else None\n",
    "        else:\n",
    "            return predict(node.right, sample) if node.right else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Prediction\n",
    "feature_names = ['Outlook', 'Temperature', 'Humidity', 'Windy']\n",
    "input_features =  ['Sunny', 'Cool', 'Normal', 'f']\n",
    "\n",
    "sample_dict = dict(zip(feature_names, input_features))\n",
    "\n",
    "predict(tree_root,sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Constraints and Prunning\n",
    "\n",
    "These **Constraints** simply tell the stopping critirea (or base case) for a recusive Tree building Function, few major ones are :\n",
    "    - Maximum Depth : Prevent the level of tree from increasing than Max depth.\n",
    "    - Minimum Sample per Split : Fixes the Minimum number of samples that should be present on a node for making a split.\n",
    "    - Minimum Sample per leaf : Fixes the Number of samples required to be at a leaf.\n",
    "    These help in preventing the Tree from Overfitting on Data, impoves it's generalization and reduces it's complexity.\n",
    "\n",
    "\n",
    "**Prunning**\n",
    "It is a Data Compression technique used in Machine Learning and Search Algortihms to reduce the Size of Tree by removing the section of Trees that are not Critical For the classification. It reduces the Complexity of the Tree and helps it not Overfit and generalize better. \n",
    "\n",
    "The **Horizen Effect:** Creating a Small Tree might not capture all the effects of different Feature, while a large one might OverFit the data. It is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error. This problem is known as the horizon effect.  \n",
    "\n",
    "It is dues to this Prunning and it's related techniques are used.\n",
    "\n",
    "Prunning Techniques :\n",
    "\n",
    "- **Pre-Prunning:** These are the Stopping Criterion or the Constraints, that tries to keep the Tree Small from the beggining.\n",
    "- **Post-Prunning:** is the most comman way, here we start with a complex tree then it's subtrees and nodes are replaced with leafs (given that it doesn't significanlty affect the accuracy of the tree). It decreases the Size of the Tree Significantly and even increases Accuracy over unseen data.\n",
    "\n",
    "Prunning Algorithms :\n",
    "\n",
    "**Reduced Error Pruning**\n",
    "\n",
    "One of the simplest forms of pruning (post-pruning). Here, first, the entire tree is grown to its full extent, and the tree almost perfectly fits the data. A validation set is prepared, and in a bottom-up way, we traverse each node and replace the subsequent subtree with a single node consisting of the most popular class. The accuracy is then calculated, and if it doesn't decrease from the original accuracy, this change is kept. This results in a simpler and more generalized tree.\n",
    "\n",
    "A few important points:\n",
    "\n",
    "- Choice of Validation Set: The validation set should be a good representation of the overall data.\n",
    "- Too Much Pruning Can Lead to Underfitting: Excessive pruning can cause the model to become too simplistic, failing to capture important patterns in the data.\n",
    "\n",
    "\n",
    "**Cost Complexity Pruning**\n",
    "\n",
    "Cost complexity pruning is a method used to prevent overfitting by balancing the complexity of the decision tree with its accuracy. Similar to Reduced Error Pruning (REP), the criteria for selecting a subtree and replacing it with a node are different. In this method, a measure called cost complexity is calculated. If its value doesn't increase significantly, then the change is kept. This is done in a bottom-up fashion, where every non-leaf node is a candidate for change. The change made is that the non-leaf node and its subtree are replaced with a leaf node representing the majority class of that node. This process is repeated iteratively until only the root node is left. Finally, using a cross-validation set, the tree with the lowest error is considered the best and is selected.\n",
    "\n",
    "The Formula for the Error calculation goes like:\n",
    "\n",
    "$$\n",
    "\\text{Cost}(T) = \\text{Err}(T) + \\alpha \\times |T|\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\text{Err}(T)$ is the proportion of samples in the dataset \\( S \\) that are incorrectly classified by the tree \\( T \\) (further this can also be based on the Loss functions -- GINI, Entropy).\n",
    "- $\\alpha$ is a hyperparameter that weights the complexity penalty (sort of like the learning rate).\n",
    "- $|T|$ represents the number of leaf nodes in the tree.\n",
    "\n",
    "Wikipedia Defines this process a little more Procedurally as:\n",
    "\n",
    "**Start with a Fully Grown Tree ($ T_0 $)**:\n",
    "\n",
    "Begin with the initial decision tree, $ T_0 $, which is fully grown and overfits the training data.\n",
    "\n",
    "**Generate a Sequence of Trees**:\n",
    "\n",
    "Create a sequence of trees $ T_0, T_1, \\ldots, T_m $, where each subsequent tree $ T_i $ is a pruned version of the previous tree $ T_{i-1} $. The sequence progresses from the fully grown tree to a tree with only the root node.\n",
    "\n",
    "**Pruning Step-by-Step**:\n",
    "\n",
    "At each iteration, consider removing a subtree $ t $ from the current tree $ T_{i-1} $.  \n",
    "Replace this subtree with a leaf node that predicts the most frequent class among the samples covered by the subtree.\n",
    "\n",
    "**Choosing Which Subtree to Prune**:\n",
    "\n",
    "Evaluate each potential subtree for removal by calculating the error rate difference:\n",
    "\n",
    "$$\n",
    "\\operatorname{err}(\\operatorname{prune}(T_{i-1}, t), S) - \\operatorname{err}(T_{i-1}, S)\n",
    "$$\n",
    "\n",
    "Normalize this difference by the change in the number of leaf nodes to determine the cost complexity:\n",
    "\n",
    "$$\n",
    "\\frac{\\operatorname{err}(\\operatorname{prune}(T_{i-1}, t), S) - \\operatorname{err}(T_{i-1}, S)}{\\left| \\operatorname{leaves}(T_{i-1}) \\right| - \\left| \\operatorname{leaves}(\\operatorname{prune}(T_{i-1}, t)) \\right|}\n",
    "$$\n",
    "\n",
    "Select the subtree that results in the smallest increase in error per leaf node removed, effectively balancing the trade-off between simplicity and accuracy.\n",
    "\n",
    "**Pruning Functionality**:\n",
    "\n",
    "Use the function $ \\operatorname{prune}(T, t) $ to systematically remove the chosen subtree from the tree $ T $.\n",
    "\n",
    "**Final Tree Selection**:\n",
    "\n",
    "After generating the sequence of pruned trees, evaluate the accuracy of each tree using a validation set or cross-validation.  \n",
    "The best tree is the one that provides the optimal balance between tree size (complexity) and predictive accuracy on the validation data.\n",
    "\n",
    "Note: The initial Set \\( S \\) that is used to calculate error in both is usually the training set, and the final selection of the tree is done on a separately prepared validation or cross-validation set.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3 (Iterative Dichotomizer 3)\n",
    "\n",
    "Typically used for natural language processing, ID3 is a decision tree-based classification algorithm. Similar to CART, it uses entropy as the loss measure instead of GINI. The overall process of building the tree is similar, involving iterations through possible splits and selecting the one with the highest information gain or lowest entropy.\n",
    "\n",
    "**Entropy:** Entropy measures the uncertainty in the data. Specifically, it represents the amount of uncertainty or impurity when predicting the class of a randomly chosen sample from the dataset. Higher entropy indicates greater uncertainty about the class of the sample.\n",
    "\n",
    "$$\n",
    "H(S) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S$ is the dataset.\n",
    "- $n$ is the number of classes in the dataset.\n",
    "- $p_i$ is the proportion of samples belonging to class $i$.\n",
    "\n",
    "**Information Gain (IG):** Information Gain measures how much the entropy is reduced by splitting the dataset on attribute $A$. It is defined as:\n",
    "\n",
    "$$\n",
    "IG(S, A) = H(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} H(S_v)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S$ is the original dataset.\n",
    "- $A$ is the attribute/feature being considered for the split.\n",
    "- $\\text{Values}(A)$ represents the set of all possible values of attribute/feature $A$.\n",
    "- $S_v$ is the subset of dataset $D$ where attribute $A$ has value $v$.\n",
    "- $|S_v|$ is the number of samples in subset $S_v$.\n",
    "- $|S|$ is the number of samples in the original dataset $S$.\n",
    "- $H(S)$ is the entropy of the original dataset.\n",
    "- $H(S_v)$ is the entropy of the subset $S_v$.\n",
    "\n",
    "Here, Information Gain measures how much the entropy is reduced by splitting the dataset on feature $A$. A higher Information Gain indicates a more informative feature for classification, meaning that this feature or features reduce the uncertainty discussed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Calculation\n",
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    This function calculates the entropy of a given dataset.\n",
    "\n",
    "    Args: \n",
    "        y (ndarray): Array of target values.\n",
    "    Returns:\n",
    "        entropy (float): Entropy value.\n",
    "    \"\"\"\n",
    "\n",
    "    class_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for cls in class_labels:\n",
    "        p_cls = len(y[y == cls]) / len(y)  # Calculating the probability of a particular class\n",
    "        if p_cls > 0:  # Check to avoid log(0)\n",
    "            entropy += - (p_cls * np.log2(p_cls)) # taking the log, Summing the probabilities\n",
    "    return entropy\n",
    "\n",
    "# Information Gain Calculation\n",
    "def information_gain_entropy(parent, l_child, r_child):\n",
    "    \"\"\"\n",
    "    This function computes the Information Gain for a particular split.\n",
    "    \n",
    "    Args:\n",
    "        parent (ndarray): Y values of the parent node.\n",
    "        l_child (ndarray): Y values of the left child node.\n",
    "        r_child (ndarray): Y values of the right child node.\n",
    "\n",
    "    Returns:\n",
    "        gain_value (float): Information Gain.\n",
    "    \"\"\"\n",
    "    # Compute entropy of the parent node\n",
    "    parent_entropy = entropy(parent)\n",
    "    \n",
    "    # Compute the weights of each child node\n",
    "    weight_left = len(l_child) / len(parent)\n",
    "    weight_right = len(r_child) / len(parent)\n",
    "    \n",
    "    # Compute entropy of the child nodes\n",
    "    left_entropy = entropy(l_child)\n",
    "    right_entropy = entropy(r_child)\n",
    "    \n",
    "    # Calculate information gain\n",
    "    gain_value = parent_entropy - (weight_left * left_entropy + weight_right * right_entropy)\n",
    "\n",
    "    return gain_value, left_entropy, right_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions created previously can be further generalized to use different loss functions by using a string argument to specify whether to use GINI or Entropy.\n",
    "\n",
    "#### With the basic classification algorithms in decision trees covered, the next step will be to explore the Decision Tree Regressor. Following that, I'll dive into more advanced algorithms and techniques to enhance decision trees, such as Random Forest Classifier, Ensemble Trees, XGBoost, AdaBoost, and more.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
